# learning

## 逻辑回归

### 1. 什么是逻辑回归


逻辑回归是一种用于分类问题的机器学习算法，特别是在二分类问题中非常常见。它通过使用 logistic 函数（也称为 sigmoid 函数）将输入特征映射到 0 和 1 之间的概率值。逻辑回归模型的目标是找到最佳的权重参数，使得模型能够准确地预测分类结果。

具体来说：
1. **输入特征**：特征向量 $ \mathbf{x} $。
2. **线性组合**： \( z = \mathbf{w} \cdot \mathbf{x} + b \)，其中 \( \mathbf{w} \) 是权重向量，\( b \) 是偏置项。
3. **logistic 函数**： \( P(y=1 \mid \mathbf{x}) = \sigma(z) = \frac{1}{1 + e^{-z}} \)，其中 \( \sigma \) 是 logistic 函数。
4. **预测**：如果 \( \sigma(z) \geq 0.5 \)，则预测为 1；否则预测为 0。

逻辑回归通过最大化似然函数或最小化交叉熵损失函数来训练模型。



### 2. 什么是Sigmoid函数

数学表达式为：

\[ \sigma(z) = \frac{1}{1 + e^{-z}} \]

Sigmoid 函数具有以下特点：

1. **S 形曲线**：函数的图像呈现出一个 S 形，因此得名 Sigmoid。
2. **输出范围**：函数的输出值范围在 0 和 1 之间，非常适合用于表示概率。
3. **可导**：Sigmoid 函数是可导的，这使得其在梯度下降等优化算法中易于使用。
4. **非线性**：Sigmoid 函数是非线性的，能够引入模型的非线性特性，增强模型的表达能力。

在逻辑回归中，Sigmoid 函数将线性组合的输出 \( z = \mathbf{w} \cdot \mathbf{x} + b \) 映射到 0 和 1 之间的概率值，从而进行二分类预测。

[补充图片]


### 3. 损失函数是什么

逻辑回归的损失函数通常是对数损失（Log Loss），也称为交叉熵损失（Cross-Entropy Loss）。逻辑回归的对数损失函数用于衡量模型预测的概率与实际标签之间的差异。对于二分类问题，对数损失函数的数学表达式为：

\[ \text{Log Loss} = -\frac{1}{n} \sum_{i=1}^{n} \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right] \]

其中：
- \( n \) 是样本数量。
- \( y_i \) 是第 \( i \) 个样本的实际标签（0或1）。
- \( \hat{y}_i \) 是第 \( i \) 个样本的预测概率，即模型输出的 \( \sigma(\mathbf{w} \cdot \mathbf{x}_i + b) \)，其中 \( \sigma \) 是 sigmoid 函数。

具体解释：
- 如果实际标签 \( y_i = 1 \)，损失函数为 \( -\log(\hat{y}_i) \)。这意味着如果预测概率 \( \hat{y}_i \) 接近 1，损失会很小；如果 \( \hat{y}_i \) 接近 0，损失会很大。
- 如果实际标签 \( y_i = 0 \)，损失函数为 \( -\log(1 - \hat{y}_i) \)。这意味着如果预测概率 \( \hat{y}_i \) 接近 0，损失会很小；如果 \( \hat{y}_i \) 接近 1，损失会很大。

损失函数的目标是最大化预测概率与真实标签之间的匹配程度，通过优化模型参数来最小化损失。

### 4.可以进行多分类吗？

逻辑回归可以用于多分类问题，通常通过两种方法实现：**一对多（One-vs-Rest, OvR）** 和 **多对多（One-vs-One, OvO）**。

**(1) 一对多（One-vs-Rest, OvR）**

在一对多方法中，逻辑回归模型为每一个类别分别训练一个二分类模型。

1. **训练**：
   - 对于每个类别 \( k \)（假设共有 \( K \) 个类别），将该类别的样本标记为 1，其他所有类别的样本标记为 0。
   - 训练一个逻辑回归模型来区分该类别和其他所有类别。
   - 重复上述步骤，直到为每个类别都训练一个模型。

2. **预测**：
   - 对于一个新的样本，使用每个模型计算该样本属于各个类别的概率。
   - 选择概率最高的类别作为最终的预测类别。

**(2) 多对多（One-vs-One, OvO）**

在多对多方法中，逻辑回归模型为每一对类别分别训练一个二分类模型。

1. **训练**：
   - 对于每一对类别 \( (i, j) \)，将类别 \( i \) 的样本标记为 1，类别 \( j \) 的样本标记为 0，训练一个逻辑回归模型来区分这两个类别。
   - 重复上述步骤，直到为所有可能的类别对都训练一个模型。对于 \( K \) 个类别，共有 \( \binom{K}{2} = \frac{K(K-1)}{2} \) 个模型。

2. **预测**：
   - 对于一个新的样本，使用每个模型进行预测，记录每个类别获得的票数。
   - 选择票数最高的类别作为最终的预测类别。


### 5.逻辑回归有什么优点

- LR能以概率的形式输出结果，而非只是0,1判定。
- 计算效率高，在较小或中型数据集上训练速度非常快。
- LR的可解释性强，权重参数可以直观地表示每个特征对分类结果的影响。
- 可以通过正则化技术（如 L1 和 L2 正则化）来处理高维数据，防止过拟合。

### 6. 逻辑回归有哪些应用

- CTR预估/推荐系统的learning to rank/各种分类场景。
- 某搜索引擎厂的广告CTR预估基线版是LR。
- 某电商搜索排序/广告CTR预估基线版是LR。
- 某电商的购物搭配推荐用了大量LR。
- 某现在一天广告赚1000w+的新闻app排序基线是LR。

### 7. 逻辑回归常用的优化方法有哪些

#### 1. 梯度下降（Gradient Descent）
梯度下降通过迭代地调整参数，沿着损失函数的负梯度方向移动，逐步接近最小值。

- **批量梯度下降（Batch Gradient Descent）**：使用所有训练样本计算梯度，更新参数。每次迭代计算成本较高，但收敛稳定。
- **随机梯度下降（Stochastic Gradient Descent, SGD）**：每次迭代只使用一个样本计算梯度，更新参数。计算成本低，收敛速度快，但可能有较大的波动。
- **小批量梯度下降（Mini-batch Gradient Descent）**：每次迭代使用一小批样本计算梯度，更新参数。结合了批量梯度下降和随机梯度下降的优点，是实际应用中最常用的变体。

#### 2. 牛顿法（Newton's Method）
牛顿法是一种二阶优化算法，利用损失函数的二阶导数（Hessian 矩阵）来加速收敛。

- **优点：** 收敛速度快，在接近最优解时表现出色。
- **缺点：** 计算 Hessian 矩阵及其逆矩阵的成本较高，适用于参数较少的情况。

#### 3. 正则化技术
正则化技术通过在损失函数中加入正则化项来防止过拟合，常用的正则化方法包括：

- **L1 正则化（Lasso）**：通过添加 L1 范数的惩罚项，可以实现稀疏解，即某些参数被置为 0。
- **L2 正则化（Ridge）**：通过添加 L2 范数的惩罚项，可以平滑参数，防止参数过大。
- **Elastic Net**：结合 L1 和 L2 正则化，提供更灵活的正则化方式。


### 8. 逻辑回归为什么要对特征进行离散化。

1. 非线性：逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合； 离散特征的增加和减少都很容易，易于模型的快速迭代； 
2. 速度快：稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展； 
3. 鲁棒性：离散化后的特征对异常数据有很强的鲁棒性。比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰； 
4. 方便交叉与特征组合：离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力； 
5. 稳定性：特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问； 
6. 简化模型：特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。

### 9. 逻辑回归的目标函数中增大L1正则化会是什么结果。

在逻辑回归的目标函数中增大L1正则化（Lasso正则化）会带来以下结果：

1. **特征稀疏化：** 增大L1正则化项会使部分模型权重（参数）被压缩到0，从而实现特征选择，仅保留对预测最重要的特征。

2. **模型简化：** 由于许多权重被置为0，模型变得更简单、更易解释，同时计算和存储需求降低。

3. **抑制过拟合：** 强化正则化可以减小模型对训练数据的过度拟合，提升在新数据上的泛化能力。

正则化过强可能导致**欠拟合**，即模型无法很好地捕捉数据的真实模式，因此需要权衡正则化强度。
