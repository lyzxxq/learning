# learning

## 逻辑回归

### 1. 什么是逻辑回归


逻辑回归是一种用于分类问题的机器学习算法，特别是在二分类问题中非常常见。它通过使用 logistic 函数（也称为 sigmoid 函数）将输入特征映射到 0 和 1 之间的概率值。逻辑回归模型的目标是找到最佳的权重参数，使得模型能够准确地预测分类结果。

具体来说：
1. **输入特征**：特征向量 $ \mathbf{x} $。
2. **线性组合**： \( z = \mathbf{w} \cdot \mathbf{x} + b \)，其中 \( \mathbf{w} \) 是权重向量，\( b \) 是偏置项。
3. **logistic 函数**： \( P(y=1 \mid \mathbf{x}) = \sigma(z) = \frac{1}{1 + e^{-z}} \)，其中 \( \sigma \) 是 logistic 函数。
4. **预测**：如果 \( \sigma(z) \geq 0.5 \)，则预测为 1；否则预测为 0。

逻辑回归通过最大化似然函数或最小化交叉熵损失函数来训练模型。



### 2. 什么是Sigmoid函数

数学表达式为：

\[ \sigma(z) = \frac{1}{1 + e^{-z}} \]

Sigmoid 函数具有以下特点：

1. **S 形曲线**：函数的图像呈现出一个 S 形，因此得名 Sigmoid。
2. **输出范围**：函数的输出值范围在 0 和 1 之间，非常适合用于表示概率。
3. **可导**：Sigmoid 函数是可导的，这使得其在梯度下降等优化算法中易于使用。
4. **非线性**：Sigmoid 函数是非线性的，能够引入模型的非线性特性，增强模型的表达能力。

在逻辑回归中，Sigmoid 函数将线性组合的输出 \( z = \mathbf{w} \cdot \mathbf{x} + b \) 映射到 0 和 1 之间的概率值，从而进行二分类预测。

[补充图片]


### 3. 损失函数是什么

逻辑回归的损失函数通常是对数损失（Log Loss），也称为交叉熵损失（Cross-Entropy Loss）。逻辑回归的对数损失函数用于衡量模型预测的概率与实际标签之间的差异。对于二分类问题，对数损失函数的数学表达式为：

\[ \text{Log Loss} = -\frac{1}{n} \sum_{i=1}^{n} \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right] \]

其中：
- \( n \) 是样本数量。
- \( y_i \) 是第 \( i \) 个样本的实际标签（0或1）。
- \( \hat{y}_i \) 是第 \( i \) 个样本的预测概率，即模型输出的 \( \sigma(\mathbf{w} \cdot \mathbf{x}_i + b) \)，其中 \( \sigma \) 是 sigmoid 函数。

具体解释：
- 如果实际标签 \( y_i = 1 \)，损失函数为 \( -\log(\hat{y}_i) \)。这意味着如果预测概率 \( \hat{y}_i \) 接近 1，损失会很小；如果 \( \hat{y}_i \) 接近 0，损失会很大。
- 如果实际标签 \( y_i = 0 \)，损失函数为 \( -\log(1 - \hat{y}_i) \)。这意味着如果预测概率 \( \hat{y}_i \) 接近 0，损失会很小；如果 \( \hat{y}_i \) 接近 1，损失会很大。

损失函数的目标是最大化预测概率与真实标签之间的匹配程度，通过优化模型参数来最小化损失。

### 4.可以进行多分类吗？

逻辑回归可以用于多分类问题，通常通过两种方法实现：**一对多（One-vs-Rest, OvR）** 和 **多对多（One-vs-One, OvO）**。

**(1) 一对多（One-vs-Rest, OvR）**

在一对多方法中，逻辑回归模型为每一个类别分别训练一个二分类模型。

1. **训练**：
   - 对于每个类别 \( k \)（假设共有 \( K \) 个类别），将该类别的样本标记为 1，其他所有类别的样本标记为 0。
   - 训练一个逻辑回归模型来区分该类别和其他所有类别。
   - 重复上述步骤，直到为每个类别都训练一个模型。

2. **预测**：
   - 对于一个新的样本，使用每个模型计算该样本属于各个类别的概率。
   - 选择概率最高的类别作为最终的预测类别。

**(2) 多对多（One-vs-One, OvO）**

在多对多方法中，逻辑回归模型为每一对类别分别训练一个二分类模型。

1. **训练**：
   - 对于每一对类别 \( (i, j) \)，将类别 \( i \) 的样本标记为 1，类别 \( j \) 的样本标记为 0，训练一个逻辑回归模型来区分这两个类别。
   - 重复上述步骤，直到为所有可能的类别对都训练一个模型。对于 \( K \) 个类别，共有 \( \binom{K}{2} = \frac{K(K-1)}{2} \) 个模型。

2. **预测**：
   - 对于一个新的样本，使用每个模型进行预测，记录每个类别获得的票数。
   - 选择票数最高的类别作为最终的预测类别。


### 5.逻辑回归有什么优点

- LR能以概率的形式输出结果，而非只是0,1判定。
- 计算效率高，在较小或中型数据集上训练速度非常快。
- LR的可解释性强，权重参数可以直观地表示每个特征对分类结果的影响。
- 可以通过正则化技术（如 L1 和 L2 正则化）来处理高维数据，防止过拟合。

### 6. 逻辑回归有哪些应用

- CTR预估/推荐系统的learning to rank/各种分类场景。
- 某搜索引擎厂的广告CTR预估基线版是LR。
- 某电商搜索排序/广告CTR预估基线版是LR。
- 某电商的购物搭配推荐用了大量LR。
- 某现在一天广告赚1000w+的新闻app排序基线是LR。

### 7. 逻辑回归常用的优化方法有哪些

#### 1. 梯度下降（Gradient Descent）
梯度下降通过迭代地调整参数，沿着损失函数的负梯度方向移动，逐步接近最小值。

- **批量梯度下降（Batch Gradient Descent）**：使用所有训练样本计算梯度，更新参数。每次迭代计算成本较高，但收敛稳定。
- **随机梯度下降（Stochastic Gradient Descent, SGD）**：每次迭代只使用一个样本计算梯度，更新参数。计算成本低，收敛速度快，但可能有较大的波动。
- **小批量梯度下降（Mini-batch Gradient Descent）**：每次迭代使用一小批样本计算梯度，更新参数。结合了批量梯度下降和随机梯度下降的优点，是实际应用中最常用的变体。

#### 2. 牛顿法（Newton's Method）
牛顿法是一种二阶优化算法，利用损失函数的二阶导数（Hessian 矩阵）来加速收敛。

- **优点：** 收敛速度快，在接近最优解时表现出色。
- **缺点：** 计算 Hessian 矩阵及其逆矩阵的成本较高，适用于参数较少的情况。

#### 3. 正则化技术
正则化技术通过在损失函数中加入正则化项来防止过拟合，常用的正则化方法包括：

- **L1 正则化（Lasso）**：通过添加 L1 范数的惩罚项，可以实现稀疏解，即某些参数被置为 0。
- **L2 正则化（Ridge）**：通过添加 L2 范数的惩罚项，可以平滑参数，防止参数过大。
- **Elastic Net**：结合 L1 和 L2 正则化，提供更灵活的正则化方式。


### 8. 逻辑回归为什么要对特征进行离散化。

1. 非线性：逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合； 离散特征的增加和减少都很容易，易于模型的快速迭代； 
2. 速度快：稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展； 
3. 鲁棒性：离散化后的特征对异常数据有很强的鲁棒性。比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰； 
4. 方便交叉与特征组合：离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力； 
5. 稳定性：特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问； 
6. 简化模型：特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。

### 9. 逻辑回归的目标函数中增大L1正则化会是什么结果。

在逻辑回归的目标函数中增大L1正则化（Lasso正则化）会带来以下结果：

1. **特征稀疏化：** 增大L1正则化项会使部分模型权重（参数）被压缩到0，从而实现特征选择，仅保留对预测最重要的特征。

2. **模型简化：** 由于许多权重被置为0，模型变得更简单、更易解释，同时计算和存储需求降低。

3. **抑制过拟合：** 强化正则化可以减小模型对训练数据的过度拟合，提升在新数据上的泛化能力。

正则化过强可能导致**欠拟合**，即模型无法很好地捕捉数据的真实模式，因此需要权衡正则化强度。


## 决策树

### 1. 什么是决策树？


决策树是一种监督学习算法，通过树状结构来表示决策过程，每个内部节点表示一个特征或属性上的测试，每个分支代表一个测试结果，每个叶节点代表一个类别或输出值。从顶部根节点开始，所有的样本聚在一起。经过根节点的划分，样本被分到不同的子节点，再根据子节点的特征进一步划分，直至所有样本被归到某一个类别（叶节点）中。

### 2. 决策树的优缺点

#### 决策树的优点

1. **易解释**：决策树的结构直观易懂，可以清晰地展示决策过程，适用于需要解释模型决策的场景。

2. **处理非线性关系**：决策树可以处理非线性关系，不需要对数据进行预处理或转换。

3. **能够处理混合类型的数据**：决策树可以处理数值型和分类型特征。

#### 决策树的缺点

1. **过拟合**：决策树容易过拟合，特别是当树的深度较大时。可以通过剪枝来缓解这一问题。

2. **不稳定性**：决策树对训练数据的小变化非常敏感，可能导致树结构的变化。


### 3. 决策树的节点划分依据是什么？请解释信息增益和基尼指数的计算方法。

决策树的节点划分依据主要用于选择最优的特征和分裂点，以最大化信息增益或最小化不纯度。常用的两种方法是信息增益（Information Gain）和基尼指数（Gini Impurity）。

#### 信息增益（Information Gain）

信息增益是基于信息熵的概念，用于衡量特征划分后的信息混乱度的减少量。信息增益越大，表示该特征对分类的贡献越大。

信息熵（Entropy）是衡量数据集混乱程度的指标，定义如下：

\[ H(S) = - \sum_{i=1}^{c} p_i \log_2 p_i \]
其中：
- \( S \) 是数据集。
- \( c \) 是数据集中的类别数。
- \( p_i \) 是第 \( i \) 类别的概率，即 \( p_i = \frac{|C_i|}{|S|} \)，其中 \( |C_i| \) 是第 \( i \) 类别的样本数，\( |S| \) 是数据集的总样本数。

信息增益表示通过某个特征划分数据集后，信息熵的减少量。计算公式如下：
\[ \text{Gain}(S, A) = H(S) - \sum_{v \in \text{values}(A)} \frac{|S_v|}{|S|} H(S_v) \]
其中：
- \( S \) 是当前数据集。
- \( A \) 是特征。
- \( \text{values}(A) \) 是特征 \( A \) 的所有取值。
- \( S_v \) 是特征 \( A \) 取值为 \( v \) 的子集。
- \( H(S_v) \) 是子集 \( S_v \) 的信息熵。

#### 基尼指数（Gini Impurity）

基尼指数用于衡量数据集的不纯度，定义如下：
\[ \text{Gini}(S) = 1 - \sum_{i=1}^{c} p_i^2 \]
其中：
- \( S \) 是数据集。
- \( c \) 是数据集中的类别数。
- \( p_i \) 是第 \( i \) 类别的概率，即 \( p_i = \frac{|C_i|}{|S|} \)，其中 \( |C_i| \) 是第 \( i \) 类别的样本数，\( |S| \) 是数据集的总样本数。

#### 基尼增益（Gini Gain）
基尼增益表示通过某个特征划分数据集后，基尼指数的减少量。计算公式如下：
\[ \text{Gini Gain}(S, A) = \text{Gini}(S) - \sum_{v \in \text{values}(A)} \frac{|S_v|}{|S|} \text{Gini}(S_v) \]
其中：
- \( S \) 是当前数据集。
- \( A \) 是特征。
- \( \text{values}(A) \) 是特征 \( A \) 的所有取值。
- \( S_v \) 是特征 \( A \) 取值为 \( v \) 的子集。
- \( \text{Gini}(S_v) \) 是子集 \( S_v \) 的基尼指数。

#### 信息增益和基尼指数的比较

- **信息增益**：
  - 对信息增益进行最大化可以导致选择具有较多值的特征，因为这样的特征可能会产生更多的子节点，从而增加信息增益。
  - 适合处理类别数较少的数据集。

- **基尼指数**：
  - 计算简单，计算效率高。
  - 对具有较多值的特征不敏感，不会过度偏向具有较多值的特征。
  - 适合处理类别数较多的数据集。


### 4. 常用的决策树有哪些？ID3、C4.5、CART有啥异同？

|  不同点   | ID3	| C4.5	| CART|
|  ----  | ----  |  ----  | ----  |
| 原则  | 信息增益最大 | 信息增益比最大 | 划分后集合基尼指数最小 |
| 用途  | 分类 | 分类 | 分类、回归 |
| 输入取值  | 离散 | 离散、连续 | 离散、连续 |
| 树结构  | 多叉树 | 多叉树 | 二叉树 |
|   | 特征在层级间不复用、对样本特征缺失值敏感 | 特征在层级间不复用 | 每个特征可被重复利用 |
|   | 分类 | 分类 | 分类、回归 |

#### 1. ID3算法

在根节点处计算信息熵，然后根据属性依次划分并计算其节点的信息熵，用根节点信息熵-属性节点的信息熵=信息增益，根据信息增益进行降序排列，排在前面的就是第一个划分属性，其后依次类推，这就得到了决策树的形状，也就是怎么“长”了。

不过，信息增益有一个问题：对可取值数目较多的属性有所偏好，例如：考虑将“编号”作为一个属性。为了解决这个问题，引出了另一个 算法C4.5。

#### 2 C4.5

为了解决信息增益的问题，引入一个信息增益率：

$Gain\_ratio(D,a)=\frac{Gain(D,a)}{IV(a)}$

其中：

$IV(a)=-\sum_{v=1}^{V}\frac{|D^v|}{|D|}log_2\frac{|D^v|}{|D|}$

属性a的可能取值数目越多(即V越大)，则IV(a)的值通常就越大。

**信息增益比本质： 是在信息增益的基础之上乘上一个惩罚参数。特征个数较多时，惩罚参数较小；特征个数较少时，惩罚参数较大。**

- 缺点：信息增益率偏向取值较少的特征。

使用信息增益率：基于以上缺点，并不是直接选择信息增益率最大的特征，而是现在候选特征中找出信息增益高于平均水平的特征，然后在这些特征中再选择信息增益率最高的特征。

#### 3 CART算法

基尼指数表示在样本集合中一个随机选中的样本被分错的概率。

因而**对于一个具有多个取值（超过2个）的特征，需要计算以每一个取值作为划分点，对样本D划分之后子集的纯度Gini(D,Ai)，(其中Ai 表示特征A的可能取值)**

然后从所有的可能划分的Gini(D,Ai)中找出Gini指数最小的划分，这个划分的划分点，便是使用特征A对样本集合D进行划分的最佳划分点。

#### 4 三种不同的决策树

- **ID3**：取值多的属性，更容易使数据更纯，其信息增益更大。

- **C4.5**：采用信息增益率替代信息增益。

- **CART**：以基尼系数替代熵，最小化不纯度，而不是最大化信息增益。


### 5. 树形结构为什么不需要归一化?

因为数值缩放不影响分裂点位置，对树模型的结构不造成影响。
按照特征值进行排序的，排序的顺序不变，那么所属的分支以及分裂点就不会有不同。而且，树模型是不能进行梯度下降的，因为构建树模型（回归树）寻找最优点时是通过寻找最优分裂点完成的，因此树模型是阶跃的，阶跃点是不可导的，并且求导没意义，也就不需要归一化。

既然树形结构（如决策树、RF）不需要归一化，那为何非树形结构比如Adaboost、SVM、LR、Knn、KMeans之类则需要归一化。

对于线性模型，特征值差别很大时，运用梯度下降的时候，损失等高线是椭圆形，需要进行多次迭代才能到达最优点。
但是如果进行了归一化，那么等高线就是圆形的，促使SGD往原点迭代，从而导致需要的迭代次数较少。

### 6. 分类决策树和回归决策树的区别

Classification And Regression Tree(CART)是决策树的一种，CART算法既可以用于创建分类树（Classification Tree），也可以用于创建回归树（Regression Tree），两者在建树的过程稍有差异。

分类决策树和回归决策树的区别主要在以下方面：

1. **目标变量类型：**
   - 分类决策树处理离散型目标变量（如分类标签：是/否、0/1）。
   - 回归决策树处理连续型目标变量（如预测数值：价格、温度）。

2. **分裂标准：**
   - 分类决策树常用**基尼指数**或**信息增益**作为分裂依据。
   - 回归决策树使用**均方误差（MSE）**或其他误差指标来确定最佳分裂点。

3. **叶节点输出：**
   - 分类决策树的叶节点输出类别标签或类别概率。
   - 回归决策树的叶节点输出连续值（如目标变量的平均值）。

4. **评价指标：**
   - 分类决策树的性能一般使用**准确率**、**ROC-AUC**等指标。
   - 回归决策树的性能通常通过**均方误差（MSE）**或**均方根误差（RMSE）**评估。

### 7. 决策树如何剪枝

决策树的剪枝基本策略有 预剪枝 (Pre-Pruning) 和 后剪枝 (Post-Pruning)。

- **预剪枝**：其中的核心思想就是，在每一次实际对结点进行进一步划分之前，先采用验证集的数据来验证如果划分是否能提高划分的准确性。如果不能，就把结点标记为叶结点并退出进一步划分；如果可以就继续递归生成节点。
- **后剪枝**：后剪枝则是先从训练集生成一颗完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来泛化性能提升，则将该子树替换为叶结点。

